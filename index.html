<!--
slidedeck: A modification of the Google IO 2012 HTML5 slide template
URL: https://github.com/rmcgibbo/slidedeck

Based on https://github.com/francescolaffi/elastic-google-io-slides, and
ultimately:

Google IO 2012 HTML5 Slide Template
Authors: Eric Bidelman <ebidel@gmail.com>
         Luke Mahe <lukem@google.com>
URL: https://code.google.com/p/io-2012-slides
-->
<!DOCTYPE html>
<html>
<head>
  <title> semantic similarity between sentences</title>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">-->
  <!--<meta name="viewport" content="width=device-width, initial-scale=1.0">-->
  <!--This one seems to work all the time, but really small on ipad-->
  <!--<meta name="viewport" content="initial-scale=0.4">-->
  <meta name="apple-mobile-web-app-capable" content="yes">
   <link rel="shortcut icon" href=" http://www.stanford.edu/favicon.ico"/> 
  <link rel="stylesheet" media="all" href="theme/css/default.css">
  <link rel="stylesheet" media="all" href="theme/css/custom.css">
  <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="theme/css/phone.css">
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->
  <script data-main="js/slides", src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.14/require.min.js"></script>


  <!-- MathJax support  -->
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    showProcessingMessages: false,
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    TeX: {
      extensions: ["color.js"]
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
  </script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <div style="display:hidden">
  \[
    \definecolor{data}{RGB}{18,110,213}
    \definecolor{unknown}{RGB}{217,86,16}
    \definecolor{learned}{RGB}{175,114,176}
  \]
  </div>

</head>

<body style="opacity: 0">

<slides class="layout-widescreen">
<slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">

    <h1> semantic similarity between sentences</h1>
    <h2> word vectors</h2>
    <p> Deven Bhooshan<br/> Founder - MindIQ Solutions Pvt Limited</p>
  </hgroup>
</slide>


<slide class="img-top-center" >
  
    <hgroup>
      <h2>about me</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>Deven</li>
<li>Co-Founder, MindIQ<ul>
<li>we use AI + Human to automate customer support</li>
<li>started in feb 2016, launched the bot builder platform in aug-2016, 10,000 + bots were build through our platform</li>
<li>featured in techinasia, venturebeat, yourstory etc - clients : smart axiata(Cambodia), hubspot, wef etcf</li>
</ul>
</li>
<li>ex-amazon employee</li>
<li>python lover, nlp enthusiast</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>Agenda</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>introduction</li>
<li>word similarity</li>
<li>sentence similarity</li>
<li>questions ?</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>key takeaways</h2>
      <h3></h3>
    </hgroup>
    <article ><ul class="build">
<li>what is semantic similarity ?</li>
<li>what are the applications of it ?</li>
<li>word representations</li>
<li>word2vec &amp; gensim apis</li>
<li>kaggle contest - quora question pairs</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>introduction</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>
<p>semantic similarity between two sentences</p>
<ul>
<li>eg: how can I uninstall snapchat ? | what is the procedure to uninstall snapchat ? | how to uninstall snapchat ?</li>
<li>these sentences are conveying the same thing/meaning/intent</li>
<li>the idea of semantic similarity between the sentences is based on the likeness of their meaning or semantic content</li>
</ul>
</li>
<li>
<p>word vectors</p>
<ul>
<li>representing words by vectors</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>word similarity</h2>
      <h3></h3>
    </hgroup>
    <article ><ul class="build">
<li>word is the most basic unit that conveys meaning</li>
<li>normally a word is representation as a string - it doesn't convey any meaning by own</li>
<li>this representation can help us find similarities in some specific use cases. cars-car, hands-hand etc</li>
<li>but what about words which are not same at character level : beautiful-alluring</li>
<li>so what can we do ?</li>
<li>we need some representation which takes into account the meaning</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words representation - vectors</h2>
      <h3></h3>
    </hgroup>
    <article ><ul class="build">
<li>we represent words by vectors</li>
<li>if suppose two words <code>beautiful</code> and <code>alluring</code> are represented by vectors <code>v1</code> and <code>v2</code>, the idea is<ul class="build">
<li><code>v1</code> and <code>v2</code> should be very similar because they convey the same meaning</li>
</ul>
</li>
<li>if suppose <code>dog</code> is represented by vector <code>v3</code>,<ul class="build">
<li><code>similarity(v1, v2) &gt; similarity(v1, v3)</code></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>word2vec & gensim</h2>
      <h3></h3>
    </hgroup>
    <article ><ul class="build">
<li>word2vec is an algorithm for constructing vector representations of words, also known as word embeddings.</li>
<li>The vector for each word is a semantic description of how that word is used in context, so two words that are used similarly in text will get similar vector represenations.</li>
<li>Once you map words into vector space, you can then use vector math to find words that have similar semantics.</li>
<li>gensim provides a nice Python implementation of Word2Vec</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words representation - examples</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>api - base <code>http://wv.mindiq.in/</code></li>
<li>vectors<ul>
<li><code>vector/?word=dog</code></li>
</ul>
</li>
<li>similarity<ul>
<li><code>similarity/?word1=kittens&amp;word2=cat</code></li>
</ul>
</li>
<li>most similar<ul>
<li><code>most_similar/?positive=mouse&amp;positive=kittens&amp;negative=cat
&amp;topn=10</code></li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words representation - examples</h2>
      <h3></h3>
    </hgroup>
    <article ><h3>Just few lines of code</h3>
<p><pre class="prettyprint" data-lang="python">
from gensim.models.keyedvectors import KeyedVectors
word_vectors = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)
....
print word_vecors['beautiful']
print word_vecors.most_similar(positive=['woman', 'king'], negative=['man'])
print word_vecors.similarity('woman', 'man')</p>
</pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words vectors - how do we produce them ?</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>first lets understand <code>Distributional Hypothesis</code><ul>
<li>words that occur in the same contexts tend to have similar meanings.</li>
<li>the underlying idea is - "a word is characterized by the company it keeps"</li>
</ul>
</li>
<li>we have three sentences<ul>
<li>india won the cricket match against sri lanka</li>
<li>australia won the cricket match against pakistan</li>
<li>bangladesh beat pakistan in the final cricket match</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words occurrences</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>we have three sentences<ul>
<li>india won the cricket match against sri lanka</li>
<li>australia won the cricket match against pakistan</li>
<li>bangladesh beat pakistan in the final cricket match</li>
</ul>
</li>
<li>look at these pairs<ul>
<li>india  - match, india - cricket</li>
<li>bangladesh - match, bangladesh - cricket</li>
<li>australia - match, australia - cricket</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words occurrences</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>india, bangladesh and australia - all in the similar context<ul>
<li>winning, loosing, playing a cricket match</li>
</ul>
</li>
<li>so this what <code>Distributional Hypothesis</code> says<ul>
<li>similar words =&gt; similar neighbors =&gt; similar vectors</li>
<li><code>india</code> - <code>bangladesh</code> - <code>australia</code> are similar(how ? country names)</li>
<li>so <code>india</code> - <code>bangladesh</code> - <code>australia</code> will have similar vectors</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>words vectors - how do we produce them ?</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>so this what <code>Distributional Hypothesis</code> says<ul>
<li>similar words =&gt; similar neighbors =&gt; similar vectors</li>
</ul>
</li>
<li>this is the basic idea of building word vectors</li>
<li>Tomas Mikolov from google created a group of models - word2vec to produce word embeddings/vectors</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>semantic similarity between sentences</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li><em>we have word vectors now</em></li>
<li>
<p>we have two sentences</p>
<ul>
<li>Obama speaks to the media in Illinois</li>
<li>The President greets the press in Chicago</li>
</ul>
</li>
<li>
<p>the above sentences are similar</p>
<ul>
<li>Chicago is a city in Illinois</li>
<li>press = media</li>
<li>speaks = greets</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>semantic similarity between sentences</h2>
      <h3></h3>
    </hgroup>
    <article ><ul class="build">
<li><em>we have word vectors now</em></li>
<li>we have two sentences which are similar</li>
<li>any algorithm which works on word strings would fail</li>
<li>what can we do ?<ul class="build">
<li>we can find the Euclidean distance in the word2vec embedding space between corresponding pairs and then add them</li>
<li>we can do the pos tagging of both the sentences and then find the similarity between verbs/nouns etc</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>lets play again - word mover distance</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>api<ul>
<li><code>word_mover_distance/?s1=Obama speaks to the media in Illinois&amp;s2=Obama greets to the press  in chicago</code></li>
</ul>
</li>
<li>that's why I love python</li>
</ul>
<pre class="prettyprint" data-lang="python">
from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)
....
print model['beautiful']
print model.most_similar(positive=['woman', 'king'], negative=['man'])
print model.similarity('woman', 'man')
<b>print model.wmdistance(
        'documents required'.split(),
        'papers needed'.split())</b>
</pre></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>semantic similarity between sentences(cont..)</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>wmd and other algorithms are good starting point for semantic similarity</li>
<li>what if we want to improve ?</li>
<li>best would be if we have some labelled data</li>
<li>something like -&gt; q1, q2, is_similar</li>
<li>we can use features like the following to train our model<ul>
<li>common words</li>
<li>wmd distance</li>
<li>etc</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>quora question pairs - kaggle contest</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>can you identify question pairs that have the same intent?</li>
<li>in simple terms<ul>
<li>given two sentences s1, s2 - we have to tell whether they are duplicates</li>
</ul>
</li>
<li>400k of labelled data</li>
<li>use above idea to build your models - use features like<ul>
<li><code>#</code> of common words</li>
<li>wmd distance etc</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>quora question pairs - sample model</h2>
      <h3></h3>
    </hgroup>
    <article ><pre class="prettyprint" data-lang="python">
from gensim.models.keyedvectors import KeyedVectors
model = KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)
....
print model['beautiful']
print model.most_similar(positive=['woman', 'king'], negative=['man'])
print model.similarity('woman', 'man')
<b>print model.wmdistance(
        'documents required'.split(),
        'papers needed'.split())</b>
</pre></article>
 
</slide>

<slide class="segue dark nobackground" >
  
    <!-- <aside class="gdbar"><img src="images/google_developers_icon_128.png"></aside> -->
    <hgroup class="auto-fadein">
      <h2>questions?</h2>
      <h3></h3>
    </hgroup>
  
</slide>

<slide  >
  
    <hgroup>
      <h2>3 similarities</h2>
      <h3></h3>
    </hgroup>
    <article ><ul>
<li>semantic similarity?</li>
<li>structure/syntactic similarity ?<ul>
<li>If sentences contains similar words as well as structure  is also similar, then there exist structural similarity between sentences.</li>
</ul>
</li>
<li>relatedness ?<ul>
<li>how related two concepts are, using any kind of relation</li>
</ul>
</li>
</ul></article>
 
</slide>

<slide  >
  
    <hgroup>
      <h2>training your own model</h2>
      <h3></h3>
    </hgroup>
    <article ><pre class="prettyprint" data-lang="python">
>>> from gensim.models import Word2Vec
>>> from nltk.corpus import brown
>>> b = Word2Vec(brown.sents())

>>> b.most_similar('money', topn=5)
[('pay', 0.6832243204116821), ('ready', 0.6152011156082153),
 ('try', 0.5845392942428589), ('care', 0.5826011896133423),
 ('move', 0.5752171277999878)]

>>> b.most_similar('great', topn=5)
[('new', 0.6999611854553223), ('experience', 0.6718623042106628),
 ('social', 0.6702290177345276), ('group', 0.6684836149215698),
 ('life', 0.6667487025260925)]

>>> b.most_similar('company', topn=5)
[('industry', 0.6164317727088928), ('technical', 0.6059585809707642),
 ('orthodontist', 0.5982754826545715), ('foamed', 0.5929019451141357),
 ('trail', 0.5763031840324402)]

</pre></article>
 
</slide>


<slide class="thank-you-slide segue nobackground">
  <!-- <aside class="gdbar right"><img src="images/google_developers_icon_128.png"></aside> -->
  <article class="flexbox vleft auto-fadein">
    <h2> Thanks everyone!</h2>
    <p></p>
  </article>
  <p data-config-contact class="auto-fadein"> <span>github</span> <a href="http://github.com/devenbhooshan">/devenbhooshan</a><br/> <span>twitter</span> <a href="http://twitter.com/devenbhooshan">@devenbhooshan</a></p>
  </p>
</slide>

<slide class="backdrop"></slide>

</slides>


<!--[if IE]>
  <script src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js"></script>
  <script>CFInstall.check({mode: 'overlay'});</script>
<![endif]-->
</body>
</html>